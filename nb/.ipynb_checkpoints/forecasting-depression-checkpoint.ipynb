{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jan. 12, 2021\n",
    "# Use the depresjon dataset to forecast health monitor activity based on existing health monitor activity\n",
    "# Split into depressed (condition) and nondepressed (control) groups, so that they are two separate models\n",
    "# Could you then forecast a new data point and classify whether they're depressed/nondepressed based on how similar their actual next day is to the forecast result from each of the models?\n",
    "\n",
    "# In a separate notebook, can also try out predicting whether someone is depressed based on their health monitor activity?\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from datetime import datetime\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Claire/code/github.com/claire.y.yang/predicting-depression/nb'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_directory = \"../data/condition\"\n",
    "control_directory = \"../data/control/\"\n",
    "\n",
    "condition_dict = {}\n",
    "control_dict = {}\n",
    "\n",
    "for filename in os.listdir(condition_directory):\n",
    "    if filename.endswith(\".csv\"): \n",
    "        condition_file_path = os.path.join(condition_directory, filename)\n",
    "        df_condition = pd.read_csv(condition_file_path)\n",
    "        subj = filename.split(\".\")[0]\n",
    "        condition_dict[subj] = df_condition\n",
    "        continue\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "for filename in os.listdir(control_directory):\n",
    "    if filename.endswith(\".csv\"): \n",
    "        control_file_path = os.path.join(control_directory, filename)\n",
    "        df_control = pd.read_csv(control_file_path)\n",
    "        subj = filename.split(\".\")[0]\n",
    "        control_dict[subj] = df_control\n",
    "        continue\n",
    "    else:\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a column in each df denoting the Y, M, d, H, M\n",
    "for cond in condition_dict:\n",
    "    cond_df = condition_dict[cond]\n",
    "    years = []\n",
    "    months = []\n",
    "    days = []\n",
    "    hours = []\n",
    "    minutes = []\n",
    "    for idx, row in cond_df.iterrows():\n",
    "        timestamp = row['timestamp']\n",
    "        parsed_timestamp = datetime.strptime(timestamp, \"%Y-%m-%d %H:%M:%S\")\n",
    "        years.append(parsed_timestamp.year)\n",
    "        months.append(parsed_timestamp.month)\n",
    "        days.append(parsed_timestamp.day)\n",
    "        hours.append(parsed_timestamp.hour)\n",
    "        minutes.append(parsed_timestamp.minute)\n",
    "        \n",
    "    cond_df['Year'] = years\n",
    "    cond_df['Month'] = months\n",
    "    cond_df['Day'] = days\n",
    "    cond_df['Hour'] = hours\n",
    "    cond_df['Minute'] = minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of days in which we do not have the full 24 hours of data\n",
    "\n",
    "new_condition_dict = {}\n",
    "for cond in condition_dict:\n",
    "    cond_df = condition_dict[cond]\n",
    "    cur_ymd = (9999, 1, 1)\n",
    "    days_to_be_removed = []\n",
    "    \n",
    "    for idx, row in cond_df.iterrows():\n",
    "        ymd = (row['Year'], row['Month'], row['Day'])\n",
    "        if cur_ymd != ymd:\n",
    "            # Check if the hour and minute are equal to 0, 0\n",
    "            if row['Hour'] != 0:\n",
    "                # Then we remove all rows containing data of this day\n",
    "                days_to_be_removed.append(ymd)\n",
    "            cur_ymd = ymd\n",
    "        \n",
    "        # If it's the last row, and the hour and minute do not equal 23, 59, then remove the whole day\n",
    "        if idx == len(cond_df) - 1:\n",
    "            if row['Hour'] != 23 and row['Minute'] != 59:\n",
    "                days_to_be_removed.append(ymd)\n",
    "        \n",
    "#     print(\"Beforehand Month, Day list:\", set(cond_df['Month']), set(cond_df['Day']))\n",
    "    # Now actually remove the day in cond_df\n",
    "    for day in days_to_be_removed:\n",
    "        cond_df = cond_df[cond_df['Day'] != day[2]]\n",
    "        \n",
    "    new_condition_dict[cond] = cond_df\n",
    "        \n",
    "#     print(\"Afterward Month, Day list:\", set(cond_df['Month']), set(cond_df['Day']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample so that it's for each half hour for each value\n",
    "# Then, create a dictionary of vectors for each condition\n",
    "# we will then use this to create the train, test sets\n",
    "\n",
    "summed_condition_dict = {}\n",
    "\n",
    "for cond in new_condition_dict:\n",
    "    new_cond_df = new_condition_dict[cond]\n",
    "    new_cond_df['timestamp'] = pd.to_datetime(new_cond_df['timestamp'], errors='coerce')\n",
    "    summed_cond_dict = dict(new_cond_df.set_index('timestamp').resample(\"30T\").sum()['activity'])\n",
    "    \n",
    "    list_vectors = []\n",
    "    cur_day = 0\n",
    "    cur_day_vector = []\n",
    "    \n",
    "    for time in summed_cond_dict:\n",
    "        day = time.day\n",
    "        activity_30_min = summed_cond_dict[time]\n",
    "        if day != cur_day:\n",
    "            if cur_day != 0:\n",
    "                list_vectors.append(cur_day_vector)\n",
    "            # It's a new day, so just reset + append to a new cur_day_vector\n",
    "            cur_day_vector = [activity_30_min]\n",
    "            cur_day = day\n",
    "        else:\n",
    "            cur_day_vector.append(activity_30_min)\n",
    "            \n",
    "    list_vectors.append(cur_day_vector) # append the last cur_day_vector\n",
    "    \n",
    "    summed_condition_dict[cond] = list_vectors\n",
    "    \n",
    "# list_vectors should be 48 x n dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, create a function that will split the dictionary into different vectors, with input look_back\n",
    "\n",
    "def split_input(dictionary, look_back=2, num_timesteps = 2):\n",
    "    X_y_list = []\n",
    "    \n",
    "    for subj in dictionary:\n",
    "        list_vectors = dictionary[subj]\n",
    "        for i in range(0, len(list_vectors) - (look_back + num_timesteps)):\n",
    "            X = list_vectors[i:i + look_back]\n",
    "            y = list_vectors[i + look_back: i + look_back + num_timesteps]\n",
    "#             print(\"This is X:\", len(X))\n",
    "#             print(\"This is y:\", len(y))\n",
    "            X_y_list.append((X, y))\n",
    "            \n",
    "    return X_y_list\n",
    "            \n",
    "data_list = split_input(summed_condition_dict, look_back=2, num_timesteps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267\n",
      "[[511, 0, 550, 1775, 47, 533, 1058, 1750, 0, 0, 1025, 652, 307, 0, 2120, 923, 697, 1072, 1968, 47, 1297, 14183, 4349, 15759, 9740, 16737, 18016, 9750, 21782, 20840, 13360, 3298, 201, 187, 355, 207, 11126, 13095, 8169, 16105, 3760, 9106, 6851, 5307, 4536, 2671, 5433, 5691], [2560, 10563, 10861, 18351, 1639, 116, 57, 48, 90, 15, 908, 748, 2801, 3061, 115, 425, 1137, 156, 0, 255, 16637, 12762, 4490, 2764, 0, 96, 4287, 3417, 6243, 5933, 6685, 4768, 19013, 23376, 126, 3700, 14886, 9783, 9661, 3603, 5435, 1973, 7071, 15374, 4812, 2276, 5615, 8740]]\n"
     ]
    }
   ],
   "source": [
    "# With lookback = 2, there are 267 samples with depression\n",
    "print(len(data_list))\n",
    "print(data_list[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we should do a random split of the sample, so that it's 75% training, 25% testing\n",
    "\n",
    "data = [i for i in range(0, len(data_list))]\n",
    "random.shuffle(data)\n",
    "\n",
    "train_split = int(len(data_list) * 0.75)\n",
    "\n",
    "train_indices = data[:train_split]\n",
    "testing_indices = data[train_split:]\n",
    "\n",
    "train_X = []\n",
    "train_y = []\n",
    "test_X = []\n",
    "test_y = []\n",
    "\n",
    "for i in range(0, len(data_list)):\n",
    "    if i in train_indices:\n",
    "        train_X.append(data_list[i][0])\n",
    "        train_y.append(data_list[i][1])\n",
    "    elif i in testing_indices:\n",
    "        test_X.append(data_list[i][0])\n",
    "        test_y.append(data_list[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(train_X)\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "test_X = np.array(test_X)\n",
    "test_y = np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's normalize the training data\n",
    "\n",
    "def normalize_data(orig_data):\n",
    "    orig_data_shape = orig_data.shape\n",
    "    reshaped_data = orig_data.reshape((orig_data_shape[0] * orig_data_shape[1], orig_data_shape[2]))\n",
    "    norm_reshaped_data = preprocessing.normalize(reshaped_data, axis=1)\n",
    "    norm_orig_data = norm_reshaped_data.reshape((orig_data_shape[0], orig_data_shape[1], orig_data_shape[2]))\n",
    "    return norm_orig_data\n",
    "\n",
    "\n",
    "norm_train_X = normalize_data(train_X)\n",
    "norm_train_y = normalize_data(train_y)\n",
    "norm_test_X = normalize_data(test_X)\n",
    "norm_test_y = normalize_data(test_y)\n",
    "\n",
    "# train_X_shape = train_X.shape\n",
    "# reshaped_train_X = train_X.reshape((train_X_shape[0] * train_X_shape[1], train_X_shape[2]))\n",
    "# norm_reshaped_train_X = preprocessing.normalize(reshaped_train_X, axis=1)\n",
    "# norm_train_X = norm_reshaped_train_X.reshape((train_X_shape[0], train_X_shape[1], train_X_shape[2]))\n",
    "\n",
    "# train_y_shape = train_y.shape\n",
    "# reshaped_train_y = train_y.reshape((train_y_shape[0] * train_y_shape[1], train_y_shape[2]))\n",
    "# norm_reshaped_train_y = preprocessing.normalize(reshaped_train_y, axis=1)\n",
    "# norm_train_y = norm_reshaped_train_y.reshape((train_y_shape[0], train_y_shape[1], train_y_shape[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_X = MinMaxScaler()\n",
    "train_X_shape = train_X.shape\n",
    "reshaped_train_X = train_X.reshape((train_X_shape[0] * train_X_shape[1], train_X_shape[2]))\n",
    "transposed_train_X = np.transpose(reshaped_train_X)\n",
    "scaled_transposed_train_X = sc_X.fit_transform(transposed_train_X)\n",
    "scaled_reshaped_train_X = np.transpose(scaled_transposed_train_X)\n",
    "scaled_train_X = scaled_reshaped_train_X.reshape((train_X_shape[0], train_X_shape[1], train_X_shape[2]))\n",
    "\n",
    "\n",
    "sc_y = MinMaxScaler()\n",
    "train_y_shape = train_y.shape\n",
    "reshaped_train_y = train_y.reshape((train_y_shape[0] * train_y_shape[1], train_y_shape[2]))\n",
    "transposed_train_y = np.transpose(reshaped_train_y)\n",
    "scaled_transposed_train_y = sc_y.fit_transform(transposed_train_y)\n",
    "scaled_reshaped_train_y = np.transpose(scaled_transposed_train_y)\n",
    "scaled_train_y = scaled_reshaped_train_y.reshape((train_y_shape[0], train_y_shape[1], train_y_shape[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 16)                4160      \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 2, 16)             0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 2, 16)             2112      \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 2, 48)             816       \n",
      "=================================================================\n",
      "Total params: 7,088\n",
      "Trainable params: 7,088\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Use Keras\n",
    "# I referenced this: https://stackabuse.com/solving-sequence-problems-with-lstm-in-keras-part-2/\n",
    "# This is a many-to-many sequence problem with multiple features\n",
    "model = Sequential()\n",
    "# encoder layer\n",
    "model.add(LSTM(16, activation='relu', input_shape=(2, 48)))\n",
    "# repeat vector\n",
    "model.add(RepeatVector(2)) # we are repeating by outputting two timesteps in the future\n",
    "# decoder layer\n",
    "model.add(LSTM(16, activation='relu', return_sequences=True))\n",
    "\n",
    "model.add(TimeDistributed(Dense(48)))\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "54/54 [==============================] - 3s 16ms/step - loss: 0.1117 - accuracy: 0.0511 - val_loss: 0.0817 - val_accuracy: 0.0500\n",
      "Epoch 2/250\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0771 - accuracy: 0.0350 - val_loss: 0.0605 - val_accuracy: 0.0375\n",
      "Epoch 3/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0591 - accuracy: 0.0595 - val_loss: 0.0572 - val_accuracy: 0.0500\n",
      "Epoch 4/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0602 - accuracy: 0.0548 - val_loss: 0.0561 - val_accuracy: 0.0250\n",
      "Epoch 5/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0610 - accuracy: 0.0427 - val_loss: 0.0554 - val_accuracy: 0.0500\n",
      "Epoch 6/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0564 - accuracy: 0.0418 - val_loss: 0.0542 - val_accuracy: 0.0250\n",
      "Epoch 7/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0552 - accuracy: 0.0649 - val_loss: 0.0554 - val_accuracy: 0.0375\n",
      "Epoch 8/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0525 - accuracy: 0.0676 - val_loss: 0.0538 - val_accuracy: 0.0250\n",
      "Epoch 9/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0541 - accuracy: 0.0480 - val_loss: 0.0541 - val_accuracy: 0.0250\n",
      "Epoch 10/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0541 - accuracy: 0.0408 - val_loss: 0.0538 - val_accuracy: 0.0375\n",
      "Epoch 11/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0525 - accuracy: 0.0854 - val_loss: 0.0536 - val_accuracy: 0.0500\n",
      "Epoch 12/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0515 - accuracy: 0.0551 - val_loss: 0.0535 - val_accuracy: 0.0250\n",
      "Epoch 13/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0526 - accuracy: 0.0347 - val_loss: 0.0529 - val_accuracy: 0.0375\n",
      "Epoch 14/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0510 - accuracy: 0.0686 - val_loss: 0.0528 - val_accuracy: 0.0250\n",
      "Epoch 15/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0523 - accuracy: 0.0813 - val_loss: 0.0527 - val_accuracy: 0.0375\n",
      "Epoch 16/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0505 - accuracy: 0.0944 - val_loss: 0.0528 - val_accuracy: 0.0125\n",
      "Epoch 17/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0515 - accuracy: 0.0619 - val_loss: 0.0529 - val_accuracy: 0.0250\n",
      "Epoch 18/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0503 - accuracy: 0.0644 - val_loss: 0.0528 - val_accuracy: 0.0250\n",
      "Epoch 19/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0506 - accuracy: 0.1014 - val_loss: 0.0540 - val_accuracy: 0.0375\n",
      "Epoch 20/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0497 - accuracy: 0.0943 - val_loss: 0.0532 - val_accuracy: 0.0250\n",
      "Epoch 21/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0477 - accuracy: 0.0936 - val_loss: 0.0536 - val_accuracy: 0.0250\n",
      "Epoch 22/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0496 - accuracy: 0.0826 - val_loss: 0.0533 - val_accuracy: 0.0125\n",
      "Epoch 23/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0476 - accuracy: 0.0642 - val_loss: 0.0541 - val_accuracy: 0.0250\n",
      "Epoch 24/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0480 - accuracy: 0.0906 - val_loss: 0.0541 - val_accuracy: 0.0500\n",
      "Epoch 25/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0478 - accuracy: 0.0538 - val_loss: 0.0539 - val_accuracy: 0.0375\n",
      "Epoch 26/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0475 - accuracy: 0.0932 - val_loss: 0.0540 - val_accuracy: 0.0250\n",
      "Epoch 27/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0486 - accuracy: 0.0881 - val_loss: 0.0540 - val_accuracy: 0.0375\n",
      "Epoch 28/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0480 - accuracy: 0.0970 - val_loss: 0.0546 - val_accuracy: 0.0250\n",
      "Epoch 29/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0462 - accuracy: 0.0812 - val_loss: 0.0540 - val_accuracy: 0.0125\n",
      "Epoch 30/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0441 - accuracy: 0.0936 - val_loss: 0.0544 - val_accuracy: 0.0375\n",
      "Epoch 31/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0455 - accuracy: 0.0981 - val_loss: 0.0547 - val_accuracy: 0.0250\n",
      "Epoch 32/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0464 - accuracy: 0.0939 - val_loss: 0.0550 - val_accuracy: 0.0250\n",
      "Epoch 33/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0456 - accuracy: 0.0845 - val_loss: 0.0548 - val_accuracy: 0.0250\n",
      "Epoch 34/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0451 - accuracy: 0.0804 - val_loss: 0.0548 - val_accuracy: 0.0125\n",
      "Epoch 35/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0466 - accuracy: 0.0636 - val_loss: 0.0552 - val_accuracy: 0.0250\n",
      "Epoch 36/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0455 - accuracy: 0.0774 - val_loss: 0.0553 - val_accuracy: 0.0375\n",
      "Epoch 37/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0438 - accuracy: 0.1025 - val_loss: 0.0557 - val_accuracy: 0.0125\n",
      "Epoch 38/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0450 - accuracy: 0.1014 - val_loss: 0.0555 - val_accuracy: 0.0125\n",
      "Epoch 39/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0453 - accuracy: 0.0782 - val_loss: 0.0558 - val_accuracy: 0.0250\n",
      "Epoch 40/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0441 - accuracy: 0.0845 - val_loss: 0.0557 - val_accuracy: 0.0125\n",
      "Epoch 41/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0440 - accuracy: 0.1317 - val_loss: 0.0557 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0466 - accuracy: 0.0636 - val_loss: 0.0557 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0441 - accuracy: 0.0965 - val_loss: 0.0565 - val_accuracy: 0.0125\n",
      "Epoch 44/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0436 - accuracy: 0.0865 - val_loss: 0.0568 - val_accuracy: 0.0125\n",
      "Epoch 45/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0452 - accuracy: 0.0884 - val_loss: 0.0566 - val_accuracy: 0.0125\n",
      "Epoch 46/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0441 - accuracy: 0.0927 - val_loss: 0.0568 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0418 - accuracy: 0.0843 - val_loss: 0.0569 - val_accuracy: 0.0250\n",
      "Epoch 48/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0439 - accuracy: 0.0919 - val_loss: 0.0572 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0421 - accuracy: 0.0654 - val_loss: 0.0572 - val_accuracy: 0.0250\n",
      "Epoch 50/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0432 - accuracy: 0.0937 - val_loss: 0.0575 - val_accuracy: 0.0250\n",
      "Epoch 51/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0431 - accuracy: 0.0578 - val_loss: 0.0572 - val_accuracy: 0.0125\n",
      "Epoch 52/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0419 - accuracy: 0.0723 - val_loss: 0.0575 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0443 - accuracy: 0.0767 - val_loss: 0.0578 - val_accuracy: 0.0250\n",
      "Epoch 54/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0416 - accuracy: 0.1206 - val_loss: 0.0580 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0435 - accuracy: 0.0823 - val_loss: 0.0584 - val_accuracy: 0.0125\n",
      "Epoch 56/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0417 - accuracy: 0.0627 - val_loss: 0.0583 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0405 - accuracy: 0.0904 - val_loss: 0.0584 - val_accuracy: 0.0125\n",
      "Epoch 58/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0401 - accuracy: 0.0793 - val_loss: 0.0587 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0399 - accuracy: 0.1035 - val_loss: 0.0588 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0406 - accuracy: 0.0810 - val_loss: 0.0582 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0394 - accuracy: 0.1194 - val_loss: 0.0584 - val_accuracy: 0.0125\n",
      "Epoch 62/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0410 - accuracy: 0.0669 - val_loss: 0.0591 - val_accuracy: 0.0125\n",
      "Epoch 63/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0428 - accuracy: 0.0783 - val_loss: 0.0586 - val_accuracy: 0.0125\n",
      "Epoch 64/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0408 - accuracy: 0.0909 - val_loss: 0.0595 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0403 - accuracy: 0.0920 - val_loss: 0.0589 - val_accuracy: 0.0250\n",
      "Epoch 66/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0399 - accuracy: 0.0802 - val_loss: 0.0593 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0409 - accuracy: 0.0915 - val_loss: 0.0601 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0399 - accuracy: 0.0596 - val_loss: 0.0592 - val_accuracy: 0.0125\n",
      "Epoch 69/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0419 - accuracy: 0.0681 - val_loss: 0.0593 - val_accuracy: 0.0125\n",
      "Epoch 70/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0403 - accuracy: 0.0591 - val_loss: 0.0600 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0407 - accuracy: 0.0750 - val_loss: 0.0606 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0413 - accuracy: 0.1015 - val_loss: 0.0594 - val_accuracy: 0.0125\n",
      "Epoch 73/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0420 - accuracy: 0.0707 - val_loss: 0.0601 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0408 - accuracy: 0.0904 - val_loss: 0.0607 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0404 - accuracy: 0.0631 - val_loss: 0.0608 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0400 - accuracy: 0.1008 - val_loss: 0.0602 - val_accuracy: 0.0125\n",
      "Epoch 77/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0393 - accuracy: 0.0699 - val_loss: 0.0604 - val_accuracy: 0.0125\n",
      "Epoch 78/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0399 - accuracy: 0.1195 - val_loss: 0.0608 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0414 - accuracy: 0.0973 - val_loss: 0.0610 - val_accuracy: 0.0125\n",
      "Epoch 80/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0385 - accuracy: 0.0889 - val_loss: 0.0615 - val_accuracy: 0.0125\n",
      "Epoch 81/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0402 - accuracy: 0.0771 - val_loss: 0.0614 - val_accuracy: 0.0125\n",
      "Epoch 82/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0407 - accuracy: 0.0913 - val_loss: 0.0617 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0411 - accuracy: 0.1369 - val_loss: 0.0615 - val_accuracy: 0.0125\n",
      "Epoch 84/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0387 - accuracy: 0.0800 - val_loss: 0.0612 - val_accuracy: 0.0125\n",
      "Epoch 85/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0413 - accuracy: 0.0911 - val_loss: 0.0615 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0397 - accuracy: 0.0933 - val_loss: 0.0621 - val_accuracy: 0.0250\n",
      "Epoch 87/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0397 - accuracy: 0.1095 - val_loss: 0.0619 - val_accuracy: 0.0250\n",
      "Epoch 88/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0374 - accuracy: 0.0776 - val_loss: 0.0623 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0386 - accuracy: 0.0642 - val_loss: 0.0629 - val_accuracy: 0.0250\n",
      "Epoch 90/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0371 - accuracy: 0.0936 - val_loss: 0.0622 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0377 - accuracy: 0.0765 - val_loss: 0.0624 - val_accuracy: 0.0125\n",
      "Epoch 92/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0383 - accuracy: 0.0895 - val_loss: 0.0622 - val_accuracy: 0.0250\n",
      "Epoch 93/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0388 - accuracy: 0.1018 - val_loss: 0.0637 - val_accuracy: 0.0250\n",
      "Epoch 94/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0391 - accuracy: 0.1099 - val_loss: 0.0629 - val_accuracy: 0.0125\n",
      "Epoch 95/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0394 - accuracy: 0.0926 - val_loss: 0.0638 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0392 - accuracy: 0.1239 - val_loss: 0.0633 - val_accuracy: 0.0250\n",
      "Epoch 97/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0385 - accuracy: 0.1203 - val_loss: 0.0639 - val_accuracy: 0.0125\n",
      "Epoch 98/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0377 - accuracy: 0.0950 - val_loss: 0.0633 - val_accuracy: 0.0250\n",
      "Epoch 99/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0391 - accuracy: 0.1095 - val_loss: 0.0638 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0376 - accuracy: 0.1187 - val_loss: 0.0638 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0373 - accuracy: 0.0989 - val_loss: 0.0634 - val_accuracy: 0.0250\n",
      "Epoch 102/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0380 - accuracy: 0.0814 - val_loss: 0.0643 - val_accuracy: 0.0125\n",
      "Epoch 103/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0375 - accuracy: 0.1109 - val_loss: 0.0650 - val_accuracy: 0.0125\n",
      "Epoch 104/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0365 - accuracy: 0.1329 - val_loss: 0.0645 - val_accuracy: 0.0125\n",
      "Epoch 105/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0375 - accuracy: 0.1284 - val_loss: 0.0644 - val_accuracy: 0.0125\n",
      "Epoch 106/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0365 - accuracy: 0.0850 - val_loss: 0.0641 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0376 - accuracy: 0.1068 - val_loss: 0.0645 - val_accuracy: 0.0125\n",
      "Epoch 108/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0383 - accuracy: 0.1160 - val_loss: 0.0640 - val_accuracy: 0.0250\n",
      "Epoch 109/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0385 - accuracy: 0.1505 - val_loss: 0.0657 - val_accuracy: 0.0125\n",
      "Epoch 110/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0395 - accuracy: 0.1048 - val_loss: 0.0651 - val_accuracy: 0.0125\n",
      "Epoch 111/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0388 - accuracy: 0.1044 - val_loss: 0.0642 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0381 - accuracy: 0.0963 - val_loss: 0.0640 - val_accuracy: 0.0125\n",
      "Epoch 113/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0369 - accuracy: 0.1279 - val_loss: 0.0649 - val_accuracy: 0.0125\n",
      "Epoch 114/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0365 - accuracy: 0.1144 - val_loss: 0.0652 - val_accuracy: 0.0250\n",
      "Epoch 115/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0360 - accuracy: 0.1645 - val_loss: 0.0652 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0366 - accuracy: 0.0996 - val_loss: 0.0644 - val_accuracy: 0.0125\n",
      "Epoch 117/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0373 - accuracy: 0.0754 - val_loss: 0.0656 - val_accuracy: 0.0125\n",
      "Epoch 118/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0379 - accuracy: 0.1192 - val_loss: 0.0651 - val_accuracy: 0.0125\n",
      "Epoch 119/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0364 - accuracy: 0.1222 - val_loss: 0.0648 - val_accuracy: 0.0250\n",
      "Epoch 120/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0368 - accuracy: 0.1348 - val_loss: 0.0659 - val_accuracy: 0.0250\n",
      "Epoch 121/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0383 - accuracy: 0.1116 - val_loss: 0.0650 - val_accuracy: 0.0125\n",
      "Epoch 122/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0368 - accuracy: 0.0842 - val_loss: 0.0663 - val_accuracy: 0.0125\n",
      "Epoch 123/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0366 - accuracy: 0.1287 - val_loss: 0.0645 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0371 - accuracy: 0.1249 - val_loss: 0.0656 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0366 - accuracy: 0.1011 - val_loss: 0.0659 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0379 - accuracy: 0.1352 - val_loss: 0.0660 - val_accuracy: 0.0125\n",
      "Epoch 127/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0366 - accuracy: 0.1054 - val_loss: 0.0659 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0371 - accuracy: 0.1463 - val_loss: 0.0655 - val_accuracy: 0.0125\n",
      "Epoch 129/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0361 - accuracy: 0.1251 - val_loss: 0.0663 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0360 - accuracy: 0.0786 - val_loss: 0.0655 - val_accuracy: 0.0125\n",
      "Epoch 131/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0361 - accuracy: 0.0965 - val_loss: 0.0667 - val_accuracy: 0.0125\n",
      "Epoch 132/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0364 - accuracy: 0.1150 - val_loss: 0.0676 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0367 - accuracy: 0.1374 - val_loss: 0.0658 - val_accuracy: 0.0250\n",
      "Epoch 134/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0353 - accuracy: 0.1149 - val_loss: 0.0659 - val_accuracy: 0.0125\n",
      "Epoch 135/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0370 - accuracy: 0.1245 - val_loss: 0.0667 - val_accuracy: 0.0125\n",
      "Epoch 136/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0372 - accuracy: 0.1208 - val_loss: 0.0656 - val_accuracy: 0.0125\n",
      "Epoch 137/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0375 - accuracy: 0.1322 - val_loss: 0.0670 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0395 - accuracy: 0.1306 - val_loss: 0.0661 - val_accuracy: 0.0250\n",
      "Epoch 139/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0359 - accuracy: 0.1313 - val_loss: 0.0671 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0360 - accuracy: 0.1230 - val_loss: 0.0657 - val_accuracy: 0.0125\n",
      "Epoch 141/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0361 - accuracy: 0.1183 - val_loss: 0.0663 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0361 - accuracy: 0.1137 - val_loss: 0.0668 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0359 - accuracy: 0.1439 - val_loss: 0.0661 - val_accuracy: 0.0125\n",
      "Epoch 144/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0356 - accuracy: 0.1494 - val_loss: 0.0665 - val_accuracy: 0.0125\n",
      "Epoch 145/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0356 - accuracy: 0.1359 - val_loss: 0.0669 - val_accuracy: 0.0125\n",
      "Epoch 146/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0352 - accuracy: 0.0850 - val_loss: 0.0663 - val_accuracy: 0.0125\n",
      "Epoch 147/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0344 - accuracy: 0.1305 - val_loss: 0.0678 - val_accuracy: 0.0125\n",
      "Epoch 148/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0353 - accuracy: 0.1320 - val_loss: 0.0665 - val_accuracy: 0.0250\n",
      "Epoch 149/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0355 - accuracy: 0.1464 - val_loss: 0.0665 - val_accuracy: 0.0125\n",
      "Epoch 150/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0355 - accuracy: 0.1457 - val_loss: 0.0667 - val_accuracy: 0.0125\n",
      "Epoch 151/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0361 - accuracy: 0.1581 - val_loss: 0.0665 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0351 - accuracy: 0.1372 - val_loss: 0.0664 - val_accuracy: 0.0125\n",
      "Epoch 153/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0370 - accuracy: 0.1370 - val_loss: 0.0674 - val_accuracy: 0.0125\n",
      "Epoch 154/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0355 - accuracy: 0.1269 - val_loss: 0.0670 - val_accuracy: 0.0125\n",
      "Epoch 155/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0351 - accuracy: 0.1190 - val_loss: 0.0672 - val_accuracy: 0.0125\n",
      "Epoch 156/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0348 - accuracy: 0.1342 - val_loss: 0.0682 - val_accuracy: 0.0250\n",
      "Epoch 157/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0342 - accuracy: 0.1477 - val_loss: 0.0674 - val_accuracy: 0.0250\n",
      "Epoch 158/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0355 - accuracy: 0.1412 - val_loss: 0.0679 - val_accuracy: 0.0125\n",
      "Epoch 159/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0359 - accuracy: 0.1389 - val_loss: 0.0666 - val_accuracy: 0.0125\n",
      "Epoch 160/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0349 - accuracy: 0.1445 - val_loss: 0.0683 - val_accuracy: 0.0125\n",
      "Epoch 161/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0352 - accuracy: 0.1383 - val_loss: 0.0674 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0348 - accuracy: 0.1637 - val_loss: 0.0680 - val_accuracy: 0.0125\n",
      "Epoch 163/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0345 - accuracy: 0.1304 - val_loss: 0.0673 - val_accuracy: 0.0125\n",
      "Epoch 164/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0348 - accuracy: 0.1388 - val_loss: 0.0684 - val_accuracy: 0.0125\n",
      "Epoch 165/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0358 - accuracy: 0.1686 - val_loss: 0.0685 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0350 - accuracy: 0.1599 - val_loss: 0.0676 - val_accuracy: 0.0125\n",
      "Epoch 167/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0340 - accuracy: 0.1737 - val_loss: 0.0694 - val_accuracy: 0.0125\n",
      "Epoch 168/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0350 - accuracy: 0.1388 - val_loss: 0.0686 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0351 - accuracy: 0.1218 - val_loss: 0.0677 - val_accuracy: 0.0125\n",
      "Epoch 170/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0346 - accuracy: 0.1326 - val_loss: 0.0685 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0352 - accuracy: 0.1395 - val_loss: 0.0677 - val_accuracy: 0.0125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0334 - accuracy: 0.1176 - val_loss: 0.0683 - val_accuracy: 0.0125\n",
      "Epoch 173/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0357 - accuracy: 0.1378 - val_loss: 0.0689 - val_accuracy: 0.0125\n",
      "Epoch 174/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0342 - accuracy: 0.1698 - val_loss: 0.0682 - val_accuracy: 0.0250\n",
      "Epoch 175/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0330 - accuracy: 0.1910 - val_loss: 0.0685 - val_accuracy: 0.0250\n",
      "Epoch 176/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0335 - accuracy: 0.1850 - val_loss: 0.0692 - val_accuracy: 0.0125\n",
      "Epoch 177/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0339 - accuracy: 0.1991 - val_loss: 0.0685 - val_accuracy: 0.0125\n",
      "Epoch 178/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0352 - accuracy: 0.1494 - val_loss: 0.0688 - val_accuracy: 0.0250\n",
      "Epoch 179/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0341 - accuracy: 0.1464 - val_loss: 0.0691 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0354 - accuracy: 0.1716 - val_loss: 0.0686 - val_accuracy: 0.0125\n",
      "Epoch 181/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0353 - accuracy: 0.1772 - val_loss: 0.0703 - val_accuracy: 0.0125\n",
      "Epoch 182/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0343 - accuracy: 0.1306 - val_loss: 0.0697 - val_accuracy: 0.0250\n",
      "Epoch 183/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0353 - accuracy: 0.1881 - val_loss: 0.0687 - val_accuracy: 0.0250\n",
      "Epoch 184/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0347 - accuracy: 0.1617 - val_loss: 0.0679 - val_accuracy: 0.0125\n",
      "Epoch 185/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0347 - accuracy: 0.1525 - val_loss: 0.0691 - val_accuracy: 0.0125\n",
      "Epoch 186/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0344 - accuracy: 0.1433 - val_loss: 0.0703 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0339 - accuracy: 0.1575 - val_loss: 0.0698 - val_accuracy: 0.0125\n",
      "Epoch 188/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0344 - accuracy: 0.1373 - val_loss: 0.0699 - val_accuracy: 0.0125\n",
      "Epoch 189/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0339 - accuracy: 0.1340 - val_loss: 0.0691 - val_accuracy: 0.0125\n",
      "Epoch 190/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0344 - accuracy: 0.1278 - val_loss: 0.0693 - val_accuracy: 0.0250\n",
      "Epoch 191/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0337 - accuracy: 0.1830 - val_loss: 0.0697 - val_accuracy: 0.0125\n",
      "Epoch 192/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0338 - accuracy: 0.1476 - val_loss: 0.0706 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0329 - accuracy: 0.1523 - val_loss: 0.0696 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0326 - accuracy: 0.1596 - val_loss: 0.0701 - val_accuracy: 0.0250\n",
      "Epoch 195/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0334 - accuracy: 0.1194 - val_loss: 0.0697 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0342 - accuracy: 0.1168 - val_loss: 0.0704 - val_accuracy: 0.0125\n",
      "Epoch 197/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0342 - accuracy: 0.1430 - val_loss: 0.0700 - val_accuracy: 0.0125\n",
      "Epoch 198/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0326 - accuracy: 0.1572 - val_loss: 0.0705 - val_accuracy: 0.0125\n",
      "Epoch 199/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0350 - accuracy: 0.1257 - val_loss: 0.0702 - val_accuracy: 0.0125\n",
      "Epoch 200/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0333 - accuracy: 0.1790 - val_loss: 0.0703 - val_accuracy: 0.0125\n",
      "Epoch 201/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0348 - accuracy: 0.1699 - val_loss: 0.0710 - val_accuracy: 0.0125\n",
      "Epoch 202/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0330 - accuracy: 0.1421 - val_loss: 0.0709 - val_accuracy: 0.0500\n",
      "Epoch 203/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0340 - accuracy: 0.1854 - val_loss: 0.0707 - val_accuracy: 0.0250\n",
      "Epoch 204/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0328 - accuracy: 0.1319 - val_loss: 0.0706 - val_accuracy: 0.0125\n",
      "Epoch 205/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0337 - accuracy: 0.1620 - val_loss: 0.0713 - val_accuracy: 0.0250\n",
      "Epoch 206/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0332 - accuracy: 0.1865 - val_loss: 0.0704 - val_accuracy: 0.0250\n",
      "Epoch 207/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0340 - accuracy: 0.1464 - val_loss: 0.0707 - val_accuracy: 0.0375\n",
      "Epoch 208/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0352 - accuracy: 0.1900 - val_loss: 0.0697 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0333 - accuracy: 0.1524 - val_loss: 0.0704 - val_accuracy: 0.0250\n",
      "Epoch 210/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0329 - accuracy: 0.1595 - val_loss: 0.0700 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0328 - accuracy: 0.2017 - val_loss: 0.0711 - val_accuracy: 0.0125\n",
      "Epoch 212/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0336 - accuracy: 0.1522 - val_loss: 0.0715 - val_accuracy: 0.0250\n",
      "Epoch 213/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0342 - accuracy: 0.1677 - val_loss: 0.0708 - val_accuracy: 0.0125\n",
      "Epoch 214/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0328 - accuracy: 0.1945 - val_loss: 0.0721 - val_accuracy: 0.0375\n",
      "Epoch 215/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0329 - accuracy: 0.1625 - val_loss: 0.0711 - val_accuracy: 0.0250\n",
      "Epoch 216/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0331 - accuracy: 0.1461 - val_loss: 0.0718 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0334 - accuracy: 0.1431 - val_loss: 0.0718 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0333 - accuracy: 0.1963 - val_loss: 0.0719 - val_accuracy: 0.0250\n",
      "Epoch 219/250\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0329 - accuracy: 0.1389 - val_loss: 0.0715 - val_accuracy: 0.0250\n",
      "Epoch 220/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0324 - accuracy: 0.1669 - val_loss: 0.0715 - val_accuracy: 0.0125\n",
      "Epoch 221/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0335 - accuracy: 0.1859 - val_loss: 0.0721 - val_accuracy: 0.0375\n",
      "Epoch 222/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0313 - accuracy: 0.1761 - val_loss: 0.0726 - val_accuracy: 0.0250\n",
      "Epoch 223/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0329 - accuracy: 0.1607 - val_loss: 0.0716 - val_accuracy: 0.0125\n",
      "Epoch 224/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0317 - accuracy: 0.1843 - val_loss: 0.0733 - val_accuracy: 0.0125\n",
      "Epoch 225/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0335 - accuracy: 0.1706 - val_loss: 0.0731 - val_accuracy: 0.0375\n",
      "Epoch 226/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0322 - accuracy: 0.1777 - val_loss: 0.0738 - val_accuracy: 0.0125\n",
      "Epoch 227/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0328 - accuracy: 0.1985 - val_loss: 0.0731 - val_accuracy: 0.0250\n",
      "Epoch 228/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0319 - accuracy: 0.1893 - val_loss: 0.0731 - val_accuracy: 0.0250\n",
      "Epoch 229/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0322 - accuracy: 0.1675 - val_loss: 0.0721 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0330 - accuracy: 0.1926 - val_loss: 0.0750 - val_accuracy: 0.0375\n",
      "Epoch 231/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0324 - accuracy: 0.2119 - val_loss: 0.0727 - val_accuracy: 0.0250\n",
      "Epoch 232/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0319 - accuracy: 0.1656 - val_loss: 0.0728 - val_accuracy: 0.0250\n",
      "Epoch 233/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0333 - accuracy: 0.2008 - val_loss: 0.0723 - val_accuracy: 0.0250\n",
      "Epoch 234/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0321 - accuracy: 0.1874 - val_loss: 0.0733 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0321 - accuracy: 0.1831 - val_loss: 0.0743 - val_accuracy: 0.0250\n",
      "Epoch 236/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0337 - accuracy: 0.1989 - val_loss: 0.0730 - val_accuracy: 0.0250\n",
      "Epoch 237/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0319 - accuracy: 0.1898 - val_loss: 0.0737 - val_accuracy: 0.0250\n",
      "Epoch 238/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0329 - accuracy: 0.1625 - val_loss: 0.0737 - val_accuracy: 0.0625\n",
      "Epoch 239/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0332 - accuracy: 0.2252 - val_loss: 0.0729 - val_accuracy: 0.0250\n",
      "Epoch 240/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0325 - accuracy: 0.1832 - val_loss: 0.0742 - val_accuracy: 0.0250\n",
      "Epoch 241/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0323 - accuracy: 0.1767 - val_loss: 0.0757 - val_accuracy: 0.0250\n",
      "Epoch 242/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0325 - accuracy: 0.1820 - val_loss: 0.0732 - val_accuracy: 0.0375\n",
      "Epoch 243/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0318 - accuracy: 0.2249 - val_loss: 0.0729 - val_accuracy: 0.0250\n",
      "Epoch 244/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0323 - accuracy: 0.1694 - val_loss: 0.0736 - val_accuracy: 0.0500\n",
      "Epoch 245/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0316 - accuracy: 0.1995 - val_loss: 0.0744 - val_accuracy: 0.0250\n",
      "Epoch 246/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0325 - accuracy: 0.1986 - val_loss: 0.0747 - val_accuracy: 0.0125\n",
      "Epoch 247/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0317 - accuracy: 0.1809 - val_loss: 0.0750 - val_accuracy: 0.0375\n",
      "Epoch 248/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0322 - accuracy: 0.1922 - val_loss: 0.0737 - val_accuracy: 0.0250\n",
      "Epoch 249/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0333 - accuracy: 0.1766 - val_loss: 0.0746 - val_accuracy: 0.0250\n",
      "Epoch 250/250\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0329 - accuracy: 0.1913 - val_loss: 0.0757 - val_accuracy: 0.0375\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(scaled_train_X, scaled_train_y, epochs=250, validation_split=0.2, verbose=1, batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Loss of Simple LSTM trained over 250 epochs on two timesteps to predict the two future timesteps')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.title(\"Loss of Simple LSTM trained over 250 epochs on two timesteps to predict the two future timesteps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3698.0527403013366\n",
      "4976.136907855071\n",
      "5370.2644497918745\n",
      "6292.0742251657775\n",
      "6037.821884532669\n",
      "5791.449188137154\n",
      "7229.5144772564445\n",
      "7144.647085414111\n",
      "4407.848582668335\n",
      "5244.607727280552\n",
      "4338.594223731138\n",
      "89.87989041208681\n",
      "65.28427533280569\n",
      "2786.7100604828674\n",
      "3141.572446143844\n",
      "4076.953245632292\n",
      "3607.1459946931973\n",
      "3019.342560895238\n",
      "3263.123067359981\n",
      "3642.7901252795373\n",
      "3545.678550069501\n",
      "2091.5167855644963\n",
      "1723.1977334676478\n",
      "1773.494682705966\n",
      "2751.4275216141714\n",
      "2977.010382392761\n",
      "3038.465380737192\n",
      "1741.894500055133\n",
      "1916.5135719386524\n",
      "1762.331548233638\n",
      "0.04096485998756802\n",
      "2378.761556523217\n",
      "3344.22569668888\n",
      "2537.0559341141557\n",
      "3302.0366710869175\n",
      "3198.4206645466784\n",
      "3096.946418274059\n",
      "2261.636047731988\n",
      "3743.158963306135\n",
      "3882.2693429816027\n",
      "3421.1288260409415\n",
      "4401.220526663285\n",
      "3788.9536958815443\n",
      "3373.448149632302\n",
      "3769.5404434400175\n",
      "1251.0005067520456\n",
      "1677.3688146805491\n",
      "1581.3708009876202\n",
      "2137.4613692331304\n",
      "2138.0130747108715\n",
      "1909.296206171019\n",
      "5061.3586198803305\n",
      "7596.605765538812\n",
      "5651.836435159657\n",
      "5915.992332603494\n",
      "5631.652727139758\n",
      "8183.619473429018\n",
      "5052.297571752582\n",
      "4679.056056103575\n",
      "2193.371492577832\n",
      "5350.436975272662\n",
      "4959.712733887536\n",
      "4473.8052130975175\n",
      "5059.097024947354\n",
      "4001.1187381937284\n",
      "4641.355800477676\n",
      "4942.160483041775\n",
      "4147.696570179673\n",
      "3803.5078538401076\n",
      "5848.6541246064235\n",
      "6926.575681629089\n",
      "5398.435026573887\n",
      "6620.903128085748\n",
      "6417.445047358807\n",
      "5902.831379129504\n",
      "7976.71975156163\n",
      "3356.797920108585\n",
      "3360.9207626833295\n",
      "3053.0720228491355\n",
      "3122.113565536053\n",
      "5026.296091480667\n",
      "4290.139510873654\n",
      "3832.2475464112645\n",
      "4389.455097098396\n",
      "4941.98262804884\n",
      "2440.283322627525\n",
      "4115.014853640189\n",
      "3855.5186203819962\n",
      "2481.0261665082385\n",
      "2609.516574891322\n",
      "2830.1364465913293\n",
      "3008.774531372877\n",
      "3627.9456734579103\n",
      "3870.7170221490264\n",
      "3159.77757907255\n",
      "5702.708837888739\n",
      "3043.274747470872\n",
      "3689.4670683810755\n",
      "5492.133632105012\n",
      "7507.725033089435\n",
      "8208.732187656633\n",
      "5838.786541461135\n",
      "3295.79643948396\n",
      "3056.217024428965\n",
      "3895.4532029255847\n",
      "17.86089063699702\n",
      "9.30113561548327\n",
      "4604.083765595093\n",
      "4854.447264291542\n",
      "4922.24625091206\n",
      "6310.590529642227\n",
      "4676.422840748767\n",
      "5343.752689218586\n",
      "6469.134410340052\n",
      "6876.359968309086\n",
      "7239.220347021795\n",
      "6403.91864978698\n",
      "8883.743060799321\n",
      "10735.464748637487\n",
      "5475.585329253133\n",
      "6158.8709598718\n",
      "5327.239523054376\n",
      "3149.7357073482094\n",
      "4502.178634365137\n",
      "4007.9994731986394\n",
      "4963.558340554647\n",
      "5527.113346342919\n",
      "4114.702714764299\n",
      "4170.857799032527\n",
      "4835.947907440071\n",
      "4368.075615324986\n",
      "5064.981194177259\n",
      "5320.467717650033\n",
      "3664.064961812373\n",
      "5683.631810529897\n",
      "5093.098978402405\n",
      "4724.035887409685\n",
      "3596.6680179919754\n",
      "181.40822248075872\n",
      "3.0413787873655234\n",
      "3.2927859044642798\n",
      "11.56000683361748\n",
      "19.637254182353313\n",
      "19.482131463709706\n",
      "33.76824853624959\n",
      "2186.1043367406915\n",
      "1934.4535859472944\n",
      "1943.1897838430855\n",
      "2230.937202369542\n",
      "2138.887903735417\n",
      "1927.1694251112017\n",
      "1820.938864463088\n",
      "1777.5074040062204\n",
      "2296.156812106496\n",
      "2367.197706390348\n",
      "2732.8025395714812\n",
      "1594.1811140149623\n",
      "2588.9531101191606\n",
      "3717.6194611067517\n",
      "3377.493458787475\n",
      "5382.000209964212\n",
      "2778.2925683732306\n",
      "3532.6367288794763\n",
      "3307.03032848642\n",
      "3500.6687110713256\n",
      "5444.158433080413\n",
      "3039.815542790237\n",
      "10765.565060781284\n",
      "7403.852728045758\n",
      "9510.016525811378\n",
      "12959.064486806175\n",
      "6467.247383482404\n",
      "6748.296177264458\n",
      "6684.954097971843\n",
      "5029.321605951308\n",
      "2773.7621953027\n",
      "3907.2712356615552\n",
      "4246.223057255766\n",
      "7049.476372280446\n",
      "7079.315863943787\n",
      "5870.449508220015\n",
      "4232.056448728783\n",
      "7544.17843579175\n",
      "6166.747308739166\n",
      "6699.100688641077\n",
      "8555.322202674632\n",
      "5543.483793028521\n",
      "8412.183037794208\n",
      "7010.7583094737565\n",
      "5945.762675724825\n",
      "5685.035467114555\n",
      "4508.3715283788615\n",
      "4905.031168519502\n",
      "5186.006760080707\n",
      "4334.84437346943\n",
      "5494.257495893411\n",
      "5636.976560652209\n",
      "4849.664966568343\n",
      "5187.70333460747\n",
      "4411.289096418884\n"
     ]
    }
   ],
   "source": [
    "# TODO: also try out the test set\n",
    "prediction = model.predict(scaled_train_X) # scaled inputs here\n",
    "\n",
    "prediction_shape = prediction.shape\n",
    "reshaped_scaled_prediction = prediction.reshape((prediction_shape[0] * prediction_shape[1], prediction_shape[2]))\n",
    "transposed_scaled_prediction = np.transpose(reshaped_scaled_prediction)\n",
    "transposed_prediction = sc_y.inverse_transform(transposed_scaled_prediction)\n",
    "reshaped_prediction = np.transpose(transposed_prediction)\n",
    "unscaled_prediction = reshaped_prediction.reshape((prediction_shape[0], prediction_shape[1], prediction_shape[2]))\n",
    "\n",
    "for j in range(0, len(train_y)):\n",
    "    MSE = mean_squared_error(train_y[j], unscaled_prediction[j])  # first 2 steps, combined\n",
    "    print(np.sqrt(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 5320.862  ,  3888.5942 ,  2225.3535 , ...,  8806.221  ,\n",
       "          8274.571  ,  5315.4727 ],\n",
       "        [ 3496.989  ,   960.16174,  -322.05447, ...,  7549.106  ,\n",
       "          6449.2095 ,  4176.372  ]],\n",
       "\n",
       "       [[ 6053.2896 ,  3841.9524 ,  2316.5483 , ...,  9547.418  ,\n",
       "          8326.01   ,  6123.5625 ],\n",
       "        [ 6203.8486 ,  3870.1052 ,  2714.313  , ...,  9427.046  ,\n",
       "          7992.4414 ,  6182.949  ]],\n",
       "\n",
       "       [[ 6100.64   ,  4606.6313 ,  3268.6968 , ..., 10057.348  ,\n",
       "          8678.555  ,  7175.0923 ],\n",
       "        [ 3039.6545 ,  1169.715  ,   177.17844, ...,  7045.6626 ,\n",
       "          5329.054  ,  5451.834  ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 4231.7886 ,  3777.97   ,  3380.6484 , ...,  4212.6895 ,\n",
       "          4687.113  ,  3160.9304 ],\n",
       "        [ 2754.8433 ,  2854.9268 ,  2868.3232 , ...,  2330.8796 ,\n",
       "          3009.496  ,  2383.4526 ]],\n",
       "\n",
       "       [[ 4302.7383 ,  4411.856  ,  2744.4446 , ...,  5386.858  ,\n",
       "          5172.237  ,  4173.622  ],\n",
       "        [ 2737.6519 ,  2990.7385 ,  1542.5688 , ...,  4034.7637 ,\n",
       "          3674.0435 ,  3427.511  ]],\n",
       "\n",
       "       [[ 4851.971  ,  4683.615  ,  4220.49   , ...,  4291.458  ,\n",
       "          3836.3923 ,  3981.9094 ],\n",
       "        [ 2655.121  ,  2596.169  ,  2699.313  , ...,  2049.7783 ,\n",
       "          1811.1841 ,  2562.0652 ]]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unscaled_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 6677,  2402,   151, ...,  4718,  4867,  4640],\n",
       "        [ 9221,   709,    59, ...,  6860,  5879,   918]],\n",
       "\n",
       "       [[ 2055,   494,   288, ..., 14954,  1117,  2515],\n",
       "        [ 1576,   293,    85, ..., 12784, 19084,  2854]],\n",
       "\n",
       "       [[ 1576,   293,    85, ..., 12784, 19084,  2854],\n",
       "        [  208,  1686,   660, ...,  5381, 16714, 13304]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  212,   301,   341, ...,  9424,  9047, 10839],\n",
       "        [ 2133,  1465,   307, ...,  3137,  2649,  2341]],\n",
       "\n",
       "       [[ 2133,  1465,   307, ...,  3137,  2649,  2341],\n",
       "        [ 2557,  1088,  2731, ...,  3519,  5803,   911]],\n",
       "\n",
       "       [[ 2557,  1088,  2731, ...,  3519,  5803,   911],\n",
       "        [ 2073,   981,  1454, ...,     0,     0,     0]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try a bidirectional LSTM\n",
    "bidir_model = Sequential()\n",
    "bidir_model.add(Bidirectional(LSTM(16, activation='relu', input_shape=(2, 48))))\n",
    "bidir_model.add(RepeatVector(2))\n",
    "bidir_model.add(Bidirectional(LSTM(16, activation='relu', return_sequences=True)))\n",
    "bidir_model.add(TimeDistributed(Dense(48)))\n",
    "\n",
    "bidir_model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "# print(bidir_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "54/54 [==============================] - 6s 17ms/step - loss: 0.1103 - accuracy: 0.0157 - val_loss: 0.0658 - val_accuracy: 0.0375\n",
      "Epoch 2/250\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.0685 - accuracy: 0.0366 - val_loss: 0.0553 - val_accuracy: 0.0125\n",
      "Epoch 3/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0606 - accuracy: 0.0554 - val_loss: 0.0546 - val_accuracy: 0.0125\n",
      "Epoch 4/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0565 - accuracy: 0.0764 - val_loss: 0.0537 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0576 - accuracy: 0.0667 - val_loss: 0.0535 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0541 - accuracy: 0.0901 - val_loss: 0.0533 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0520 - accuracy: 0.0890 - val_loss: 0.0527 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0540 - accuracy: 0.1116 - val_loss: 0.0522 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0544 - accuracy: 0.0941 - val_loss: 0.0525 - val_accuracy: 0.0500\n",
      "Epoch 10/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0501 - accuracy: 0.0862 - val_loss: 0.0518 - val_accuracy: 0.0125\n",
      "Epoch 11/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0501 - accuracy: 0.0972 - val_loss: 0.0522 - val_accuracy: 0.0375\n",
      "Epoch 12/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0499 - accuracy: 0.1013 - val_loss: 0.0519 - val_accuracy: 0.0500\n",
      "Epoch 13/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0508 - accuracy: 0.0936 - val_loss: 0.0538 - val_accuracy: 0.0500\n",
      "Epoch 14/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0479 - accuracy: 0.0939 - val_loss: 0.0517 - val_accuracy: 0.0375\n",
      "Epoch 15/250\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0481 - accuracy: 0.1027 - val_loss: 0.0524 - val_accuracy: 0.0125\n",
      "Epoch 16/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0459 - accuracy: 0.0942 - val_loss: 0.0522 - val_accuracy: 0.0500\n",
      "Epoch 17/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0453 - accuracy: 0.1078 - val_loss: 0.0524 - val_accuracy: 0.0500\n",
      "Epoch 18/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0470 - accuracy: 0.0646 - val_loss: 0.0536 - val_accuracy: 0.0500\n",
      "Epoch 19/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0448 - accuracy: 0.1004 - val_loss: 0.0529 - val_accuracy: 0.0250\n",
      "Epoch 20/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0448 - accuracy: 0.0874 - val_loss: 0.0531 - val_accuracy: 0.0250\n",
      "Epoch 21/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0440 - accuracy: 0.0674 - val_loss: 0.0533 - val_accuracy: 0.0375\n",
      "Epoch 22/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0443 - accuracy: 0.0902 - val_loss: 0.0532 - val_accuracy: 0.0500\n",
      "Epoch 23/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0440 - accuracy: 0.0624 - val_loss: 0.0558 - val_accuracy: 0.0375\n",
      "Epoch 24/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0436 - accuracy: 0.0401 - val_loss: 0.0538 - val_accuracy: 0.0250\n",
      "Epoch 25/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0448 - accuracy: 0.1018 - val_loss: 0.0549 - val_accuracy: 0.0125\n",
      "Epoch 26/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0436 - accuracy: 0.1222 - val_loss: 0.0549 - val_accuracy: 0.0125\n",
      "Epoch 27/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0410 - accuracy: 0.1030 - val_loss: 0.0554 - val_accuracy: 0.0125\n",
      "Epoch 28/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0418 - accuracy: 0.0713 - val_loss: 0.0552 - val_accuracy: 0.0250\n",
      "Epoch 29/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0417 - accuracy: 0.1238 - val_loss: 0.0549 - val_accuracy: 0.0250\n",
      "Epoch 30/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0403 - accuracy: 0.1167 - val_loss: 0.0554 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0400 - accuracy: 0.1379 - val_loss: 0.0559 - val_accuracy: 0.0125\n",
      "Epoch 32/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0393 - accuracy: 0.1644 - val_loss: 0.0562 - val_accuracy: 0.0125\n",
      "Epoch 33/250\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0383 - accuracy: 0.1254 - val_loss: 0.0562 - val_accuracy: 0.0250\n",
      "Epoch 34/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0386 - accuracy: 0.1135 - val_loss: 0.0573 - val_accuracy: 0.0125\n",
      "Epoch 35/250\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0376 - accuracy: 0.1304 - val_loss: 0.0569 - val_accuracy: 0.0125\n",
      "Epoch 36/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0387 - accuracy: 0.1574 - val_loss: 0.0581 - val_accuracy: 0.0125\n",
      "Epoch 37/250\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0361 - accuracy: 0.1379 - val_loss: 0.0579 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0383 - accuracy: 0.1402 - val_loss: 0.0581 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/250\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.0387 - accuracy: 0.1226 - val_loss: 0.0575 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/250\n",
      "12/54 [=====>........................] - ETA: 0s - loss: 0.0373 - accuracy: 0.2007"
     ]
    }
   ],
   "source": [
    "bidir_history = bidir_model.fit(scaled_train_X, scaled_train_y, epochs=250, validation_split=0.2, verbose=1, batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bidir_history.history['loss'])\n",
    "plt.title(\"Loss of Bidirectional LSTM trained over 250 epochs on two timesteps to predict the two future timesteps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bidir_history.history['accuracy'])\n",
    "plt.title(\"Accuracy of Bidirectional LSTM trained over 250 epochs on two timesteps to predict the two future timesteps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try to predict the test set and compare?\n",
    "test_pred_y = bidir_model.predict(norm_test_X, verbose=0)\n",
    "print(test_pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
